# -*- coding: utf-8 -*-
"""Clothing recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lUrl-QYpfda7DY66ZsOHn-4ddh4jjKLn
"""

# https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST
from torchvision import datasets
from torchvision.transforms import ToTensor   # img -> tensor
import matplotlib.pyplot as plt
import random
from torch.utils.data import DataLoader  # batch
from torch import nn
import torch
from tqdm.auto import tqdm


train_data = datasets.FashionMNIST(
    root="image",
    train=True,       # training set
    download=True,
    transform=ToTensor())   # image -> tensor

test_data = datasets.FashionMNIST(
    root="image",
    train=False,      # testing set
    download=True,
    transform=ToTensor())   # image -> tensor

# show images
random_idx = random.randint(0, len(train_data)-1)
img, label = train_data[random_idx]
class_names = train_data.classes
class_names[label]
plt.imshow(img.permute(1, 2, 0), cmap="gray")  # tensor[1, 28, 28] -> plt[28, 28, 1]
plt.title(class_names[label])

# When the amount of data is too large, classification is necessary.
# Because when there are a large number of samples, if the computer needs to process each sample individually before updating, the speed will be too slow.
len(train_data), len(test_data)

BATCH_SIZE = 32  # 32/64/128/256

train_dataloader = DataLoader(
    train_data,
    batch_size=BATCH_SIZE,
    shuffle=True)   # Shuffle the training set data.

test_dataloader = DataLoader(
    test_data,
    batch_size=BATCH_SIZE,
    shuffle=False)   # Shuffle the testing set data.

len(train_dataloader), len(test_dataloader)

# iterable = can inspect in a loop
#qq = [1, 2, 3]
#qq_iterator = iter(qq)
#next(qq_iterator) 1 -> 2 -> 3

x_first_batch, y_first_batch = next(iter(train_dataloader))   # x=graph, y=label
#x_first_batch.shape, y_first_batch.shape

random_idx = random.randint(0, len(x_first_batch)-1)
img, label = x_first_batch[random_idx], y_first_batch[random_idx]

plt.imshow(img.permute(1, 2, 0))
plt.title(class_names[label])

# graph have 3 dim -> faltten = 1 dim -> nn
#x_first_batch[0].shape
f = nn.Flatten(start_dim=0, end_dim=-1)
f(x_first_batch[0]).shape   # 28pixel*28pixel = 784pixel

# Define the nn
# too complex!
# class ImageClassificationModel(nn.Module):
#   def __init__(self):
#     super().__init__()
#     self.flatten = nn.Flatten(start_dim=0, end_dim=-1)
#     self.linear_layer = nn.Linear(in_features=784, out_features=10) # in_features = 784 ->  nn.Flatten = 1*18*18 = 784, out_feature = 10 -> There are ten labels in FasionMNIST
#     self.softmax = nn.Softmax(dim=0)

#   def forward(self, x):
#     return self.softmax(self.linear_layer(self.flatten(x)))

# Another syntax
# single graph -> batch
class ImageClassificationModel(nn.Module):
  def __init__(self, input_shape, output_shape):
    super().__init__()
    self.layer_stack = nn.Sequential(
      nn.Flatten(start_dim=1, end_dim=-1),
      nn.Linear(in_features=input_shape, out_features=output_shape), # in_features = 784 ->  nn.Flatten = 1*18*18 = 784, out_feature = 10 -> There are ten labels in FasionMNIST
      # nn.Softmax(dim=1)   # nn.CrossEntropyLoss()
    )

  def forward(self, x):
    return self.layer_stack(x)

x_first_batch, y_first_batch = next(iter(train_dataloader))   # x=graph, y=label

# https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax
torch.manual_seed(87)
model = ImageClassificationModel(28*28, 10)
y_pred = model(x_first_batch)
#y_pred.sum(dim=1) # Ensure the sum is one
#y_pred.argmax(dim=1)

# https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss
# cost function
cost_fn = nn.CrossEntropyLoss()
y_pred = model(x_first_batch)
cost = cost_fn(y_pred, y_first_batch) # do Softmax two times
print(cost)
print(model.state_dict())

optimizer = torch.optim.SGD(params=model.parameters(), lr=0.01)
optimizer.zero_grad()
cost.backward()
optimizer.step()

y_pred = model(x_first_batch)
cost = cost_fn(y_pred, y_first_batch)
print(cost)
print(model.state_dict())

def accuracy_fn(y_pred, y_true):
  correct_num = (y_pred==y_true).sum()
  acc = correct_num / len(y_true) * 100

  return acc

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

epochs = 3

for epoch in tqdm(range(epochs)):
  print(f"Epoch:  {epoch}\n---------")

  train_cost = 0
  train_acc = 0
  for batch, (x, y) in enumerate(train_dataloader):
    x = x.to(device)  # to cuda
    y = y.to(device)

    model.train()

    y_pred = model(x)

    cost = cost_fn(y_pred, y)

    train_cost += cost
    train_acc += accuracy_fn(y_pred.argmax(dim=1), y)

    optimizer.zero_grad()

    cost.backward()

    optimizer.step()

    if batch%500==0:
      print(f"now data: {batch*len(x)}/{len(train_data)} data")

  train_cost /= len(train_dataloader)
  train_acc /= len(train_dataloader)

  test_cost = 0
  test_acc = 0
  model.eval()
  with torch.inference_mode():
    for x, y in test_dataloader:
      x = x.to(device)  # to cuda
      y = y.to(device)

      test_pred = model(x)
      test_cost += cost_fn(test_pred, y)
      test_acc += accuracy_fn(test_pred.argmax(dim=1), y)

    test_cost /= len(test_dataloader)
    test_acc /= len(test_dataloader)


  print(f"\nTrain Cost:   {train_cost:.4f},   Train Acc:  {train_acc:.4f} ")
  print(f"\nTest Cost:   {test_cost:.4f},   Test Acc:  {test_acc:.4f} \n")
