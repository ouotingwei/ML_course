# -*- coding: utf-8 -*-
"""Clothing recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lUrl-QYpfda7DY66ZsOHn-4ddh4jjKLn
"""

# https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST
from torchvision import datasets
from torchvision.transforms import ToTensor   # img -> tensor
import matplotlib.pyplot as plt
import random
from torch.utils.data import DataLoader  # batch
from torch import nn
import torch


train_data = datasets.FashionMNIST(
    root="image",
    train=True,       # training set
    download=True,
    transform=ToTensor())   # image -> tensor

test_data = datasets.FashionMNIST(
    root="image",
    train=False,      # testing set
    download=True,
    transform=ToTensor())   # image -> tensor

# show images
random_idx = random.randint(0, len(train_data)-1)
img, label = train_data[random_idx]
class_names = train_data.classes
class_names[label]
plt.imshow(img.permute(1, 2, 0), cmap="gray")  # tensor[1, 28, 28] -> plt[28, 28, 1]
plt.title(class_names[label])

# When the amount of data is too large, classification is necessary.
# Because when there are a large number of samples, if the computer needs to process each sample individually before updating, the speed will be too slow.
len(train_data), len(test_data)

BATCH_SIZE = 32  # 32/64/128/256

train_dataloader = DataLoader(
    train_data,
    batch_size=BATCH_SIZE,
    shuffle=True)   # Shuffle the training set data.

test_dataloader = DataLoader(
    test_data,
    batch_size=BATCH_SIZE,
    shuffle=False)   # Shuffle the testing set data.

len(train_dataloader), len(test_dataloader)

# iterable = can inspect in a loop
#qq = [1, 2, 3]
#qq_iterator = iter(qq)
#next(qq_iterator) 1 -> 2 -> 3

x_first_batch, y_first_batch = next(iter(train_dataloader))   # x=graph, y=label
#x_first_batch.shape, y_first_batch.shape

random_idx = random.randint(0, len(x_first_batch)-1)
img, label = x_first_batch[random_idx], y_first_batch[random_idx]

plt.imshow(img.permute(1, 2, 0))
plt.title(class_names[label])

# graph have 3 dim -> faltten = 1 dim -> nn
#x_first_batch[0].shape
f = nn.Flatten(start_dim=0, end_dim=-1)
f(x_first_batch[0]).shape   # 28pixel*28pixel = 784pixel

# Define the nn
# too complex!
# class ImageClassificationModel(nn.Module):
#   def __init__(self):
#     super().__init__()
#     self.flatten = nn.Flatten(start_dim=0, end_dim=-1)
#     self.linear_layer = nn.Linear(in_features=784, out_features=10) # in_features = 784 ->  nn.Flatten = 1*18*18 = 784, out_feature = 10 -> There are ten labels in FasionMNIST
#     self.softmax = nn.Softmax(dim=0)

#   def forward(self, x):
#     return self.softmax(self.linear_layer(self.flatten(x)))

# Another syntax
# single graph -> batch
class ImageClassificationModel(nn.Module):
  def __init__(self, input_shape, output_shape):
    super().__init__()
    self.layer_stack = nn.Sequential(
      nn.Flatten(start_dim=1, end_dim=-1),
      nn.Linear(in_features=input_shape, out_features=output_shape), # in_features = 784 ->  nn.Flatten = 1*18*18 = 784, out_feature = 10 -> There are ten labels in FasionMNIST
      nn.Softmax(dim=1)
    )

  def forward(self, x):
    return self.layer_stack(x)

x_first_batch, y_first_batch = next(iter(train_dataloader))   # x=graph, y=label

# https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax
torch.manual_seed(87)
model = ImageClassificationModel(28*28, 10)
y_pred = model(x_first_batch)
#y_pred.sum(dim=1) # Ensure the sum is one
#y_pred.argmax(dim=1)
